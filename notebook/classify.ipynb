{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cea512b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, random, pandas as pd\n",
    "random.seed(7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "658a5899",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "575b6eb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/yvesgreatti/github/drug_discovery/notebook'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3538321b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entry</th>\n",
       "      <th>uniprot_description</th>\n",
       "      <th>GPT_description</th>\n",
       "      <th>all_description</th>\n",
       "      <th>effect_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q8N884-D95A</td>\n",
       "      <td>No effect on type I IFN and RSAD2 induction. N...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No effect on type I IFN and RSAD2 induction. N...</td>\n",
       "      <td>No effect on type I IFN and RSAD2 induction. N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P06276-L153F</td>\n",
       "      <td>In BCHED; seems to cause reduced expression of...</td>\n",
       "      <td>The mutation in the BCHE gene leads to reduced...</td>\n",
       "      <td>In BCHED; seems to cause reduced expression of...</td>\n",
       "      <td>In BCHED; seems to cause reduced expression of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P63096-E245L</td>\n",
       "      <td>Enhances interaction (inactive GDP-bound) with...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Enhances interaction (inactive GDP-bound) with...</td>\n",
       "      <td>Enhances interaction (inactive GDP-bound) with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O35244-S32A</td>\n",
       "      <td>Abolishes lipid binding.</td>\n",
       "      <td>Increased Prdx6 alpha-helical content, key rol...</td>\n",
       "      <td>Abolishes lipid binding. Increased Prdx6 alpha...</td>\n",
       "      <td>Abolishes lipid binding. Increased Prdx6 alpha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P80365-R337C</td>\n",
       "      <td>In AME; decreased half-life from 21 to 4 hours...</td>\n",
       "      <td>This mutation has been discovered in a consang...</td>\n",
       "      <td>In AME; decreased half-life from 21 to 4 hours...</td>\n",
       "      <td>In AME; decreased half-life from 21 to 4 hours...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          entry                                uniprot_description  \\\n",
       "0   Q8N884-D95A  No effect on type I IFN and RSAD2 induction. N...   \n",
       "1  P06276-L153F  In BCHED; seems to cause reduced expression of...   \n",
       "2  P63096-E245L  Enhances interaction (inactive GDP-bound) with...   \n",
       "3   O35244-S32A                           Abolishes lipid binding.   \n",
       "4  P80365-R337C  In AME; decreased half-life from 21 to 4 hours...   \n",
       "\n",
       "                                     GPT_description  \\\n",
       "0                                                NaN   \n",
       "1  The mutation in the BCHE gene leads to reduced...   \n",
       "2                                                NaN   \n",
       "3  Increased Prdx6 alpha-helical content, key rol...   \n",
       "4  This mutation has been discovered in a consang...   \n",
       "\n",
       "                                     all_description  \\\n",
       "0  No effect on type I IFN and RSAD2 induction. N...   \n",
       "1  In BCHED; seems to cause reduced expression of...   \n",
       "2  Enhances interaction (inactive GDP-bound) with...   \n",
       "3  Abolishes lipid binding. Increased Prdx6 alpha...   \n",
       "4  In AME; decreased half-life from 21 to 4 hours...   \n",
       "\n",
       "                                         effect_text  \n",
       "0  No effect on type I IFN and RSAD2 induction. N...  \n",
       "1  In BCHED; seems to cause reduced expression of...  \n",
       "2  Enhances interaction (inactive GDP-bound) with...  \n",
       "3  Abolishes lipid binding. Increased Prdx6 alpha...  \n",
       "4  In AME; decreased half-life from 21 to 4 hours...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load the dataset\n",
    "usecols = [\"entry\",\"uniprot_description\",\"GPT_description\",\"all_description\"]\n",
    "df = pd.read_csv(\"../mutadescribe_data/structural_split/train.csv\", usecols=usecols, low_memory=False)\n",
    "\n",
    "# Prefer enriched text, fallback to raw\n",
    "df[\"effect_text\"] = df[\"all_description\"].fillna(df[\"uniprot_description\"]).fillna(df[\"GPT_description\"])\n",
    "df = df.dropna(subset=[\"effect_text\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f53b4fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Lightweight polarity rules (negation-aware)\n",
    "NEG = [r\"loss[- ]of[- ]function\", r\"\\blof\\b\", r\"decreas(?:e|es|ed|ing)\", r\"reduc(?:e|es|ed|ing)\",\n",
    "       r\"impair(?:s|ed|ing|ment)\", r\"inactivat(?:e|es|ed|ing)\", r\"disrupt(?:s|ed|ion)\",\n",
    "       r\"abolish(?:es|ed|ing)?\", r\"destabiliz(?:e|es|ed|ing)\", r\"misfold(?:s|ed|ing)?\",\n",
    "       r\"defect(?:ive)?\", r\"deleterious\", r\"inhibit(?:s|ed|ing|ion)\"]\n",
    "POS = [r\"gain[- ]of[- ]function\", r\"\\bgof\\b\", r\"increas(?:e|es|ed|ing)\", r\"enhanc(?:e|es|ed|ing)\",\n",
    "       r\"activat(?:e|es|ed|ing)\", r\"stabiliz(?:e|es|ed|ing)\", r\"improv(?:e|es|ed|ing)\",\n",
    "       r\"up[- ]?regulat(?:e|es|ed|ing)\", r\"beneficial\", r\"protect(?:ive|ion|s|ed)?\"]\n",
    "NEU = [r\"no (?:significant|measurable|observable) (?:change|effect|difference)\",\n",
    "       r\"does not (?:affect|alter|impact)\", r\"not (?:affect|alter|impact)(?:ed|ing)?\",\n",
    "       r\"unchang(?:ed|ing)\", r\"wild[- ]type(?:[- ]like)?\", r\"\\bWT[- ]?like\\b\",\n",
    "       r\"comparable to (?:wt|wild[- ]type)\", r\"\\bneutral\\b\", r\"\\btolerated\\b\"]\n",
    "\n",
    "NEGATORS = {\"no\",\"not\",\"without\",\"lack\",\"lacks\",\"lacking\",\"fails\",\"failed\",\"absence\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "298812a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _has_negator(left, n=4):\n",
    "    toks = re.findall(r\"[a-zA-Z']+\", left.lower())\n",
    "    return any(t in NEGATORS for t in toks[-n:])\n",
    "\n",
    "def _count(text, pats):\n",
    "    c=0\n",
    "    for p in pats:\n",
    "        for m in re.finditer(p, text, flags=re.I):\n",
    "            if not _has_negator(text[:m.start()]):\n",
    "                c+=1\n",
    "    return c\n",
    "\n",
    "def label(text):\n",
    "    t = \" \".join(str(text).split())\n",
    "    if _count(t, NEU)>0: return \"Not significant\"\n",
    "    neg, pos = _count(t, NEG), _count(t, POS)\n",
    "    if neg==0 and pos==0: return \"Unknown\"\n",
    "    return \"Malignant\" if neg>pos else \"Benign\"\n",
    "\n",
    "df[\"label\"] = df[\"effect_text\"].apply(label)\n",
    "\n",
    "# 3) Build balanced, short few-shots\n",
    "def pick_shots(df, per_class=4, max_words=35):\n",
    "    shots = []\n",
    "    for cls in [\"Malignant\",\"Benign\",\"Not significant\",\"Unknown\"]:\n",
    "        cand = df[(df[\"label\"]==cls) & (df[\"effect_text\"].str.split().str.len()<=max_words)]\n",
    "        # prefer sentences with a clear single clause\n",
    "        cand = cand[cand[\"effect_text\"].str.count(r\"[.;]\")<=2]\n",
    "        take = cand.sample(min(per_class, len(cand)), random_state=7)[\"effect_text\"].tolist()\n",
    "        shots += [(t, cls) for t in take]\n",
    "    random.shuffle(shots)\n",
    "    return shots\n",
    "\n",
    "shots = pick_shots(df, per_class=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8eab7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You label the functional polarity of a protein mutation from text.\n",
      "- Malignant = detrimental/negative effect (LoF, decreased activity, destabilization, inhibition, misfolding…)\n",
      "- Benign = beneficial/positive effect (GoF, increased activity, stabilization, activation, rescue…)\n",
      "- Not significant = explicitly no/negligible effect (WT-like, unchanged, no significant change).\n",
      "- Unknown = insufficient/contradictory.\n",
      "Respect negation (“no decrease” is NOT negative). Output exactly one: Malignant | Benign | Not significant | Unknown.\n",
      "\n",
      "FEW-SHOTS:\n",
      "TEXT: Increased binding affinity to curdlan compared to the wild-type.\n",
      "LABEL: Not significant\n",
      "\n",
      "TEXT: Gain of activity.\n",
      "LABEL: Unknown\n",
      "\n",
      "TEXT: Enables self-association and NF-kappa-B inhibition by B14. The mutation in the B14 protein promotes its binding to IKKβ and activating NF-κB-dependent gene expression.\n",
      "LABEL: Benign\n",
      "\n",
      "TEXT: No effect on BMAL1 binding.\n",
      "LABEL: Unknown\n",
      "\n",
      "TEXT: In RP62; results in a complete loss of kinase activity compared to wild-type.\n",
      "LABEL: Not significant\n",
      "\n",
      "TEXT: Increased VASH1 tyrosine carboxypeptidase activity on alpha-tubulin.\n",
      "LABEL: Benign\n",
      "\n",
      "TEXT: The mutation at codon 100 changes the amino acid residue from phenylalanine (F) to serine (S).\n",
      "LABEL: Unknown\n",
      "\n",
      "TEXT: Reduces the nuclear migrating activity.\n",
      "LABEL: Malignant\n",
      "\n",
      "TEXT: Abolishes protein binding. The mutation in the Abp1p protein causes a change in its structure and binding specificity.\n",
      "LABEL: Malignant\n",
      "\n",
      "TEXT: Decreased protein expression compared to wild type when transfected in HEK293T cells.\n",
      "LABEL: Not significant\n",
      "\n",
      "TEXT: Decreases transcriptional activity. The point mutation within the inhibitory domain of NGFI-A resulted in a 17-fold decrease in activity.\n",
      "LABEL: Malignant\n",
      "\n",
      "TEXT: Increases histone acetylase activity. Decreases sensitivity to methyl methane sulfonate and hydroxyurea (genotoxic stress).\n",
      "LABEL: Benign\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SYSTEM = \"\"\"You label the functional polarity of a protein mutation from text.\n",
    "- Malignant = detrimental/negative effect (LoF, decreased activity, destabilization, inhibition, misfolding…)\n",
    "- Benign = beneficial/positive effect (GoF, increased activity, stabilization, activation, rescue…)\n",
    "- Not significant = explicitly no/negligible effect (WT-like, unchanged, no significant change).\n",
    "- Unknown = insufficient/contradictory.\n",
    "Respect negation (“no decrease” is NOT negative). Output exactly one: Malignant | Benign | Not significant | Unknown.\"\"\"\n",
    "print(SYSTEM)\n",
    "print(\"\\nFEW-SHOTS:\")\n",
    "for t, y in shots:\n",
    "    print(f\"TEXT: {t}\\nLABEL: {y}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "793db39a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/yvesgreatti/github/drug_discovery/notebook'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4be659",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "HF_TOKEN = \"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\"\n",
    "#HF_TOKEN = os.environ.get(\"HF_TOKEN\")  # set this first if the repo is gated\n",
    "CACHE_DIR = \"../hf_models\"  # choose where files go on disk\n",
    "\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\n",
    "    MODEL_ID, use_fast=True, token=HF_TOKEN, cache_dir=CACHE_DIR\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    token=HF_TOKEN,\n",
    "    device_map=\"cpu\",                # <— CPU only\n",
    "    torch_dtype=torch.float32,       # <— CPU needs fp32\n",
    "    low_cpu_mem_usage=True,          # <— reduces peak load\n",
    "    cache_dir=CACHE_DIR,\n",
    ").eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f9040cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c970c4ca175d4ff1b5bbbbe56c8c8036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Meta-Llama-3-8B-Instruct-Q4_K_M.gguf:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../hf_models/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import os\n",
    "HF_TOKEN = \"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\"\n",
    "CACHE_DIR = \"../hf_models\"  # choose where files go on disk\n",
    "\n",
    "local_path = hf_hub_download(\n",
    "    repo_id=\"bartowski/Meta-Llama-3-8B-Instruct-GGUF\",  # or QuantFactory/*\n",
    "    filename=\"Meta-Llama-3-8B-Instruct-Q4_K_M.gguf\",\n",
    "    token=HF_TOKEN,\n",
    "    local_dir=CACHE_DIR\n",
    ")\n",
    "print(local_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d202c84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_cpp import Llama\n",
    "\n",
    "# llm = Llama(\n",
    "#     model_path=\"../hf_models/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf\",\n",
    "#     n_ctx=4096,\n",
    "#     n_threads=8,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "381310d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c686a5f641174b2ca59fdf841bbde662",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# pip install -U transformers accelerate bitsandbytes torch\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from huggingface_hub import login\n",
    "token = \"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\"\n",
    "login(token=token)\n",
    "\n",
    "# 0) MODEL\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"   # or a biomed LLM\n",
    "labels   = [\"Malignant\", \"Benign\", \"Not significant\", \"Unknown\"]\n",
    "\n",
    "bnb = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "\n",
    "CACHE_DIR = \"../hf_models\"  # choose where files go on disk\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\n",
    "    MODEL_ID, use_fast=True, token=token, cache_dir=CACHE_DIR\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    token=token,\n",
    "    device_map=\"cpu\",                # <— CPU only\n",
    "    torch_dtype=torch.float32,       # <— CPU needs fp32\n",
    "    low_cpu_mem_usage=True,          # <— reduces peak load\n",
    "    cache_dir=CACHE_DIR,\n",
    ").eval()\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     MODEL_ID, device_map=\"auto\", quantization_config=bnb\n",
    "# ).eval()\n",
    "\n",
    "# You already have:\n",
    "#   SYSTEM  -> string\n",
    "#   shots   -> list[tuple[text,label]] like: [(\"text...\", \"Malignant\"), ...]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a0abe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _chat_messages(effect_text: str):\n",
    "    \"\"\"\n",
    "    Build a proper chat log:\n",
    "      [system] rules\n",
    "      [user]/[assistant] few-shot pairs\n",
    "      [user]  query ending with 'LABEL:'\n",
    "    \"\"\"\n",
    "    msgs = [{\"role\": \"system\", \"content\": SYSTEM}]\n",
    "    for t, y in shots:\n",
    "        msgs.append({\"role\": \"user\", \"content\": f\"TEXT: {t}\\nLABEL:\"})\n",
    "        msgs.append({\"role\": \"assistant\", \"content\": y})\n",
    "    msgs.append({\"role\": \"user\", \"content\": f\"TEXT: {effect_text}\\nLABEL:\"})\n",
    "    return msgs\n",
    "\n",
    "def build_prompt(effect_text: str) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Turn the chat messages into model-ready token ids using the tokenizer's\n",
    "    chat template (best for Llama-style instruct models).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # returns token ids when tokenize=True\n",
    "        ids = tok.apply_chat_template(\n",
    "            _chat_messages(effect_text),\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "    except Exception:\n",
    "        # Fallback to a plain string if the model has no chat template\n",
    "        few = \"\".join([f\"\\nTEXT: {x}\\nLABEL: {y}\" for x, y in shots])\n",
    "        plain = (\n",
    "            f\"<s>[SYSTEM]\\n{SYSTEM}\\n[/SYSTEM]\\n\"\n",
    "            f\"[USER]\\nClassify the following text.{few}\\n\\nTEXT: {effect_text}\\nLABEL: [/USER]\\n[ASSISTANT]\\n\"\n",
    "        )\n",
    "        ids = tok(plain, return_tensors=\"pt\", add_special_tokens=False).input_ids\n",
    "    return ids.to(model.device)\n",
    "\n",
    "@torch.inference_mode()\n",
    "def label_logprob(prompt_ids: torch.Tensor, label: str) -> float:\n",
    "    \"\"\"\n",
    "    log P(label | prompt) as a sum of next-token logprobs over the label tokens.\n",
    "    \"\"\"\n",
    "    lab_ids = tok(\" \" + label, return_tensors=\"pt\", add_special_tokens=False).input_ids.to(model.device)\n",
    "    input_ids = torch.cat([prompt_ids, lab_ids], dim=1)\n",
    "    attn_mask = torch.ones_like(input_ids)\n",
    "\n",
    "    out = model(input_ids=input_ids, attention_mask=attn_mask).logits[:, :-1, :]\n",
    "    targets = input_ids[:, 1:]\n",
    "\n",
    "    Lp = prompt_ids.size(1)\n",
    "    Ll = lab_ids.size(1)\n",
    "    # positions where the label tokens are predicted\n",
    "    logprobs = out[:, Lp-1:Lp+Ll-1, :].log_softmax(dim=-1)\n",
    "    tgt = targets[:, Lp-1:Lp+Ll-1]\n",
    "    return float(logprobs.gather(-1, tgt.unsqueeze(-1)).squeeze(-1).sum().item())\n",
    "\n",
    "# def label_logprob(llm: Llama, prompt_text: str, label: str, *, add_bos: bool = True) -> float:\n",
    "#     \"\"\"\n",
    "#     Return log P(label | prompt) by summing next-token logprobs over label tokens.\n",
    "#     Works by feeding the concatenated prompt+label with echo=True and reading\n",
    "#     the per-token logprobs for the label slice.\n",
    "#     \"\"\"\n",
    "#     # Ensure label tokenization is aligned with Llama tokenizer conventions\n",
    "#     label_text = \" \" + label\n",
    "\n",
    "#     # Tokenize to find the slice that corresponds to the label\n",
    "#     prompt_toks = llm.tokenize(prompt_text.encode(\"utf-8\"), add_bos=add_bos)\n",
    "#     full_toks   = llm.tokenize((prompt_text + label_text).encode(\"utf-8\"), add_bos=add_bos)\n",
    "#     Lp = len(prompt_toks)\n",
    "#     Ll = len(full_toks) - Lp\n",
    "\n",
    "#     # Ask llama.cpp for per-token logprobs for *prompt+label*\n",
    "#     # max_tokens=0 + echo=True means: don't generate, just score what we sent.\n",
    "#     out = llm.create_completion(\n",
    "#         prompt=prompt_text + label_text,\n",
    "#         max_tokens=0,\n",
    "#         echo=True,\n",
    "#         logprobs=1,   # return per-token logprobs\n",
    "#     )\n",
    "\n",
    "#     token_logprobs = out[\"choices\"][0][\"logprobs\"][\"token_logprobs\"]\n",
    "#     # token_logprobs aligns with the tokenized full sequence (may have a leading None for BOS)\n",
    "#     start = len(token_logprobs) - Ll\n",
    "#     label_slice = token_logprobs[start:]\n",
    "\n",
    "#     # Sum over the label tokens; guard against any None values (e.g., BOS)\n",
    "#     return float(sum(lp for lp in label_slice if lp is not None))\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def classify(text: str, margin_threshold: float = 1.0) -> dict:\n",
    "    \"\"\"\n",
    "    Deterministic zero/few-shot classifier.\n",
    "    margin_threshold controls 'Unknown' gating when top-2 scores are close.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return {\"label\": \"Unknown\", \"margin\": 0.0, \"scores\": {}}\n",
    "\n",
    "    prompt_ids = build_prompt(text)\n",
    "    scores = {lab: label_logprob(prompt_ids, lab) for lab in labels}\n",
    "    best = max(scores, key=scores.get)\n",
    "    sorted_vals = sorted(scores.values(), reverse=True)\n",
    "    margin = sorted_vals[0] - sorted_vals[1] if len(sorted_vals) > 1 else float(\"inf\")\n",
    "    label = best if margin >= margin_threshold else \"Unknown\"\n",
    "    return {\"label\": label, \"margin\": margin, \"scores\": scores}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587719c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = classify(\"Variant reduces catalytic activity and destabilizes the protein.\")\n",
    "    print(res)  # {'label': 'Malignant', 'margin': ..., 'scores': {...}}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
